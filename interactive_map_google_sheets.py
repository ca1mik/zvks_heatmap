"""
Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚: interactive_map_google_sheets.py

â€¢ Ğ•Ğ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾ Ğ·Ğ°Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· GoogleÂ Sheets
â€¢ ĞšÑÑˆĞ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Nominatim Ğ»Ğ¸ÑˆĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ·
â€¢ Ğ‘ÑƒĞ´ĞµÑ‚ ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ĞºĞ°Ñ€Ñ‚Ñƒ (Folium) Ñ:
    â€“ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ğ¾Ğ¹ (HeatMap)
    â€“ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ (Ğ²Ğ¾Ğ´Ğ°, ĞºĞ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ‚.Ğ´.)
â€¢ ĞĞ° Ğ²Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ´Ğ°Ñ‚ Ğ¸ ÑĞ¿Ğ¸ÑĞ¾Ğº ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ â€” Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ·Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ· cron / GitHubÂ Actions.

ĞŸĞµÑ€ĞµĞ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼:
1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ ÑĞµÑ€Ğ²Ğ¸Ñâ€‘Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚ Ğ² GoogleÂ Cloud, ÑĞºĞ°Ñ‡Ğ°Ğ¹Ñ‚Ğµ JSONâ€‘ĞºĞ»ÑÑ‡ Ğ¸ Ğ½Ğ°Ğ·Ğ¾Ğ²Ğ¸Ñ‚Ğµ ĞµĞ³Ğ¾ service_account.json.
2. ĞŸĞ¾Ğ´ĞµĞ»Ğ¸Ñ‚ĞµÑÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†ĞµĞ¹ Ñ Ğ°Ğ´Ñ€ĞµÑĞ¾Ğ¼ ÑĞµÑ€Ğ²Ğ¸Ñâ€‘Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚Ğ° (share -> Editor).
3. Ğ£ĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ ID Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ GOOGLE_SHEET_ID.
4. Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ ĞµÑÑ‚ÑŒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸:
   "created_at" (Ğ´Ğ°Ñ‚Ğ°/Ğ²Ñ€ĞµĞ¼Ñ Ğ·Ğ°ÑĞ²ĞºĞ¸),
   "category" (Ñ‚Ğ¸Ğ¿ Ñ€Ğ°Ğ±Ğ¾Ñ‚),
   "Ğ£Ğ»Ğ¸Ñ†Ğ°", "Ğ”Ğ¾Ğ¼", "Ğ’ÑĞµĞ³Ğ¾" (ĞºĞ¾Ğ»â€‘Ğ²Ğ¾ Ğ·Ğ°ÑĞ²Ğ¾Ğº).  ĞŸÑ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½ÑƒĞ¹Ñ‚Ğµ Ğ½Ğ¸Ğ¶Ğµ.
5. pip install -U gspread gspread_dataframe oauth2client pandas geopy folium

Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°:
python interactive_map_google_sheets.py \
       --from 2025-06-01 --to 2025-06-30 --cat Ğ²Ğ¾Ğ´Ğ° ĞºĞ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

Ğ¡Ğ¾Ğ·Ğ´Ğ°ÑÑ‚ Ñ„Ğ°Ğ¹Ğ» map_2025-06-01_2025-06-30.html Ñ€ÑĞ´Ğ¾Ğ¼ ÑĞ¾ ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ¼.
"""

import argparse
import datetime as dt
import os
import pickle
from pathlib import Path
from typing import List, Optional

import folium
import gspread
import pandas as pd
from folium.plugins import HeatMap, MarkerCluster, MiniMap, Fullscreen
from geopy.distance import geodesic
from geopy.extra.rate_limiter import RateLimiter
from geopy.geocoders import Nominatim
from gspread_dataframe import get_as_dataframe

# â”€â”€â”€ 1) ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ â€” Ğ¿Ğ¾Ğ¿Ñ€Ğ°Ğ²ÑŒÑ‚Ğµ Ğ¿Ğ¾Ğ´ ÑĞµĞ±Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GOOGLE_SHEET_ID = "PASTE_SHEET_ID_HERE"  # ğŸ‘‰ Ğ’Ğ°Ñˆ ID Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ (Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ÑÑ Ñ‡Ğ°ÑÑ‚ÑŒ URL)
SERVICE_ACCOUNT_JSON = "service_account.json"  # ğŸ‘‰ ĞŸÑƒÑ‚ÑŒ Ğº JSONâ€‘ĞºĞ»ÑÑ‡Ñƒ
WORKSHEET_NAME: Optional[str] = None  # None = Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ»Ğ¸ÑÑ‚

CENTER = (55.8437, 48.5066)            # Ğ¦ĞµĞ½Ñ‚Ñ€ ĞºĞ°Ñ€Ñ‚Ñ‹ (Ğ—ĞµĞ»ĞµĞ½Ğ¾Ğ´Ğ¾Ğ»ÑŒÑĞº)
MAX_DIST_KM = 10                        # ĞÑ‚ÑĞµÑ‡ĞºĞ° Ğ¾Ñ‚ Ñ‡ÑƒĞ¶Ğ¸Ñ… Ğ°Ğ´Ñ€ĞµÑĞ¾Ğ²
GEOCACHE_FILE = "geocache.pkl"         # ĞšÑƒĞ´Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹
OUTPUT_PATTERN = "map_{from_d}_{to_d}.html"  # Ğ¨Ğ°Ğ±Ğ»Ğ¾Ğ½ Ğ¸Ğ¼ĞµĞ½Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ°

# ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ² GoogleÂ Sheets (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ¼ĞµĞ½ÑÑ‚ÑŒ, ĞµÑĞ»Ğ¸ Ñƒ Ğ²Ğ°Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ)
COL_STREET = "Ğ£Ğ»Ğ¸Ñ†Ğ°"
COL_HOUSE = "Ğ”Ğ¾Ğ¼"
COL_COUNT = "Ğ’ÑĞµĞ³Ğ¾"
COL_DATE = "created_at"
COL_CAT = "category"

# â”€â”€â”€ 2) ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº GoogleÂ Sheets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_sheet_df(sheet_id: str, cred_json: str, worksheet_name: Optional[str]) -> pd.DataFrame:
    """Ğ¡Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ»Ğ¸ÑÑ‚ GoogleÂ Sheets Ğ² DataFrame"""
    gc = gspread.service_account(filename=cred_json)
    sh = gc.open_by_key(sheet_id)
    ws = sh.worksheet(worksheet_name) if worksheet_name else sh.sheet1
    df = get_as_dataframe(ws, evaluate_formulas=True, header=0)
    return df

# â”€â”€â”€ 3) Ğ“ĞµĞ¾ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ĞºÑÑˆĞµĞ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_cache(path: str):
    if Path(path).exists():
        with open(path, "rb") as f:
            return pickle.load(f)
    return {}

def save_cache(cache: dict, path: str):
    with open(path, "wb") as f:
        pickle.dump(cache, f)

def smart_geocode(street: str, house: str, geocode_fn, cache: dict):
    key = f"{street}_{house}"
    if key in cache:
        return cache[key]

    address = f"{street}, {house}, Ğ—ĞµĞ»ĞµĞ½Ğ¾Ğ´Ğ¾Ğ»ÑŒÑĞº, Ğ ĞµÑĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ° Ğ¢Ğ°Ñ‚Ğ°Ñ€ÑÑ‚Ğ°Ğ½, Ğ Ğ¾ÑÑĞ¸Ñ"
    loc = geocode_fn(address)
    if loc and geodesic(CENTER, (loc.latitude, loc.longitude)).km <= MAX_DIST_KM:
        cache[key] = (loc.latitude, loc.longitude)
    else:
        cache[key] = None
    return cache[key]

# â”€â”€â”€ 4) ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def prepare_dataframe(df_raw: pd.DataFrame) -> pd.DataFrame:
    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ ÑƒĞ»Ğ¸Ñ†Ñ‹/Ğ´Ğ¾Ğ¼Ğ°
    df = df_raw.copy()
    df = df[df[COL_STREET].notna() & df[COL_HOUSE].notna()]

    # Ğ¿Ñ€Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²
    df[COL_STREET] = df[COL_STREET].astype(str)
    df[COL_HOUSE] = df[COL_HOUSE].astype(str)
    df[COL_COUNT] = df[COL_COUNT].fillna(0).astype(int)
    df[COL_DATE] = pd.to_datetime(df[COL_DATE])
    df[COL_CAT] = df[COL_CAT].fillna("ĞĞµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¾")
    return df

# â”€â”€â”€ 5) Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ°Ñ‚Ğµ Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def filter_df(df: pd.DataFrame, date_from: dt.date, date_to: dt.date, cats: Optional[List[str]]):
    mask = (df[COL_DATE] >= pd.Timestamp(date_from)) & (df[COL_DATE] <= pd.Timestamp(date_to) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1))
    if cats:
        mask &= df[COL_CAT].isin(cats)
    return df[mask].reset_index(drop=True)

# â”€â”€â”€ 6) ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_map(df: pd.DataFrame) -> folium.Map:
    m = folium.Map(location=CENTER, zoom_start=13, tiles="CartoDB Positron", control_scale=True)
    MiniMap(toggle_display=True).add_to(m)
    Fullscreen().add_to(m)

    # Ğ¢ĞµĞ¿Ğ»Ğ¾Ğ²Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ°
    if not df.empty:
        heat_data = df[["lat", "lon", COL_COUNT]].values.tolist()
        HeatMap(heat_data, radius=20, blur=15, min_opacity=0.3).add_to(folium.FeatureGroup("ğŸ”¥ HeatMap", show=True).add_to(m))

    # ĞšĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼
    clusters = {}
    for cat in df[COL_CAT].unique():
        fg = folium.FeatureGroup(cat, show=False).add_to(m)
        clusters[cat] = MarkerCluster().add_to(fg)

    for _, r in df.iterrows():
        mc = clusters[r[COL_CAT]]
        folium.Marker(
            [r.lat, r.lon],
            popup=f"{r[COL_STREET]} {r[COL_HOUSE]}<br>{r[COL_CAT]}<br>{r[COL_COUNT]} ÑˆÑ‚.<br>{r[COL_DATE].date()}",
            icon=folium.Icon(color="blue", icon="info-sign"),
        ).add_to(mc)

    # ĞŸĞ¾Ğ´Ğ³Ğ¾Ğ½ÑĞµĞ¼ Ğ·ÑƒĞ¼ Ğ¿Ğ¾Ğ´ Ğ²ÑĞµ Ñ‚Ğ¾Ñ‡ĞºĞ¸
    if not df.empty:
        sw = [df.lat.min(), df.lon.min()]
        ne = [df.lat.max(), df.lon.max()]
        m.fit_bounds([sw, ne], padding=(50, 50))

    folium.LayerControl(collapsed=False).add_to(m)
    return m

# â”€â”€â”€ 7) ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_map(date_from: str, date_to: str, categories: Optional[List[str]] = None, worksheet_name: Optional[str] = WORKSHEET_NAME):
    raw_df = fetch_sheet_df(GOOGLE_SHEET_ID, SERVICE_ACCOUNT_JSON, worksheet_name)
    df = prepare_dataframe(raw_df)

    # Ğ“ĞµĞ¾ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ Ñ ĞºÑÑˆĞµĞ¼
    cache = load_cache(GEOCACHE_FILE)
    geolocator = Nominatim(user_agent="zelenod_map")
    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)
    coords = df.apply(lambda r: smart_geocode(r[COL_STREET], r[COL_HOUSE], geocode, cache), axis=1)
    df["lat"] = coords.apply(lambda c: c[0] if c else None)
    df["lon"] = coords.apply(lambda c: c[1] if c else None)
    save_cache(cache, GEOCACHE_FILE)

    df = df.dropna(subset=["lat", "lon"])

    df_filt = filter_df(df, dt.date.fromisoformat(date_from), dt.date.fromisoformat(date_to), categories)

    m = build_map(df_filt)
    out_name = OUTPUT_PATTERN.format(from_d=date_from, to_d=date_to)
    m.save(out_name)
    print(f"âœ… ĞšĞ°Ñ€Ñ‚Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ² {out_name} â€” {len(df_filt)} Ñ‚Ğ¾Ñ‡ĞµĞº")

# â”€â”€â”€ 8) CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ·Ğ°ÑĞ²Ğ¾Ğº Ğ¸Ğ· GoogleÂ Sheets")
    parser.add_argument("--from", "-f", dest="d_from", required=True, help="ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ° YYYY-MM-DD")
    parser.add_argument("--to", "-t", dest="d_to", required=True, help="ĞšĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ° YYYY-MM-DD")
    parser.add_argument("--cat", "-c", dest="cats", nargs="*", help="Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ²ÑĞµ)")
    parser.add_argument("--sheet", "-s", dest="sheet", help="ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¸ÑÑ‚Ğ° (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹)")

    args = parser.parse_args()
    generate_map(args.d_from, args.d_to, args.cats, args.sheet)
